{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prithvirao/AIpractise1/blob/master/NLP_project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019bd668",
      "metadata": {
        "id": "019bd668"
      },
      "outputs": [],
      "source": [
        " import tweepy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tweepy==3.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "7DOllOQxzonX",
        "outputId": "c839a4a4-f673-4a89-8f91-610833ae9b74"
      },
      "id": "7DOllOQxzonX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweepy==3.7.0\n",
            "  Downloading tweepy-3.7.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.8/dist-packages (from tweepy==3.7.0) (1.7.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tweepy==3.7.0) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.8/dist-packages (from tweepy==3.7.0) (2.25.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tweepy==3.7.0) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.11.1->tweepy==3.7.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.11.1->tweepy==3.7.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.11.1->tweepy==3.7.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.11.1->tweepy==3.7.0) (4.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->tweepy==3.7.0) (3.2.2)\n",
            "Installing collected packages: tweepy\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "Successfully installed tweepy-3.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tweepy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kyhp-Z6lznCC"
      },
      "id": "Kyhp-Z6lznCC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8be5918",
      "metadata": {
        "id": "b8be5918"
      },
      "outputs": [],
      "source": [
        "# set up Twitter API credentials\n",
        "consumer_key = \"skakU2irbEBd97RvhpBsW0sXL\"\n",
        "consumer_secret = \"Bgb8TuOlhCB0nRBpyJCMtKOLkvhuSW7D0V1EoJzf7p7McFwknf\"\n",
        "access_token = \"880363396016869376-fFRlVcD2l7sMDmpGZw3yZelKxijcwEP\"\n",
        "access_token_secret = \"l0QpT9WuWXHCV4on1C20irIzXYrtxIoHoJXHtSh7vxG4j\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3686fa",
      "metadata": {
        "id": "dc3686fa"
      },
      "outputs": [],
      "source": [
        "# set up API authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35ebc17",
      "metadata": {
        "id": "f35ebc17"
      },
      "outputs": [],
      "source": [
        "# set up API client\n",
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707ccfa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "707ccfa4",
        "outputId": "e8094b19-08b5-4f86-c168-8d94d1202b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a0530d4",
      "metadata": {
        "id": "9a0530d4"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove urls\n",
        "    text = re.sub(r'\\#\\w+', '', text)  # remove hashtags\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "TjoqRaWo1Y7X"
      },
      "id": "TjoqRaWo1Y7X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737b2980",
      "metadata": {
        "id": "737b2980"
      },
      "outputs": [],
      "source": [
        "# Extract relevant tweets based on keywords and retweet count\n",
        "keywords = ['earthquake']\n",
        "retweet_threshold = 5\n",
        "# Define the date range\n",
        "#start_date = datetime.utcnow() - timedelta(days=5)\n",
        "#end_date = datetime.utcnow()\n",
        "today = datetime.datetime.now().date()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-U6k5Nb0DOM",
        "outputId": "ac87693f-64c5-4ffe-e207-a6880adfe207"
      },
      "id": "0-U6k5Nb0DOM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "GDf5KA7i0eq2",
        "outputId": "47e723a6-12de-44bc-eb59-10a49989bba7"
      },
      "id": "GDf5KA7i0eq2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.26.1\n",
            "Uninstalling transformers-4.26.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.8/dist-packages/transformers-4.26.1.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/transformers/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.26.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb27ec66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "cb27ec66",
        "outputId": "a657ff22-b8b7-46e9-a318-2c7de85db55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of earthquake related tweets for 2023-02-23:\n",
            "; a d o Ã©, e. : - s Ã   )'en?!... & Â» / _ ---   de l y ant  au eau  tub tub ul  in t  do re n f er  and oxioxi aliment  for ÃŸ procÃ©dÃ© essai  to Ã©preuve he actu m c eur  ( );\n",
            "\n",
            "\n",
            "Summary of earthquake related tweets for 2023-02-22:\n",
            "; a d o Ã©, e. : - s Ã   )'en?!... & Â» / _ ---   de l y ant  au eau  tub tub ul  in t  do re n f er  and oxioxi aliment  for ÃŸ procÃ©dÃ© essai  to Ã©preuve he actu m c eur  ( );\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c9cae42e21c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msummary_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m             )\n\u001b[1;32m   1473\u001b[0m             \u001b[0;31m# 13. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1475\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2720\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2723\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1689\u001b[0m             \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    date = today - datetime.timedelta(days=i+1)\n",
        "    since_date = str(date) + ' 00:00:00'\n",
        "    until_date = str(date) + ' 23:59:59'\n",
        "    tweets = []\n",
        "    for keyword in keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang='en', tweet_mode='extended', since=since_date, until=until_date).items(100):\n",
        "            text = tweet.full_text\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            if tweet.retweet_count >= retweet_threshold:\n",
        "                tweets.append((preprocessed_text, tweet.retweet_count))\n",
        "            else:\n",
        "                tweets.append((preprocessed_text, 0))\n",
        "\n",
        "    # Rank and summarize the relevant tweets using a machine learning-based model (T5)\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "    sorted_tweets = sorted(tweets, key=lambda x: x[1], reverse=True)  # sort by retweet count\n",
        "\n",
        "    input_text = ' '.join([tweet[0] for tweet in sorted_tweets])\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=200, min_length=50, no_repeat_ngram_size=2)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f'Summary of earthquake related tweets for {since_date.split()[0]}:')\n",
        "    print(summary)\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097d6a6d",
      "metadata": {
        "id": "097d6a6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "giving summary for each day"
      ],
      "metadata": {
        "id": "IiydfQmPIRVN"
      },
      "id": "IiydfQmPIRVN"
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# set up Twitter API credentials\n",
        "consumer_key = \"skakU2irbEBd97RvhpBsW0sXL\"\n",
        "consumer_secret = \"Bgb8TuOlhCB0nRBpyJCMtKOLkvhuSW7D0V1EoJzf7p7McFwknf\"\n",
        "access_token = \"880363396016869376-fFRlVcD2l7sMDmpGZw3yZelKxijcwEP\"\n",
        "access_token_secret = \"l0QpT9WuWXHCV4on1C20irIzXYrtxIoHoJXHtSh7vxG4j\"\n",
        "\n",
        "# set up API authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# set up API client\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove urls\n",
        "    text = re.sub(r'\\#\\w+', '', text)  # remove hashtags\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
        "    return text\n",
        "\n",
        "# Extract relevant tweets based on keywords and dates\n",
        "start_date = datetime.now() - timedelta(days=5)\n",
        "end_date = datetime.now()\n",
        "\n",
        "# Collect tweets for each day\n",
        "for i in range((end_date - start_date).days):\n",
        "    day = start_date + timedelta(days=i)\n",
        "    date_str = day.strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    # Extract relevant tweets based on keywords\n",
        "    keywords = ['earthquake']\n",
        "    tweets = []\n",
        "    for keyword in keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang='en', tweet_mode='extended', until=date_str).items(20):\n",
        "            text = tweet.full_text\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            tweets.append(preprocessed_text)\n",
        "    \n",
        "    # Rank and summarize the relevant tweets using a machine learning-based model (T5)\n",
        "    input_text = ' '.join(tweets)\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=200, min_length=50, no_repeat_ngram_size=2)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"Summary for {date_str}:\")\n",
        "    print(summary)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvO4tDbp4Fj6",
        "outputId": "c6804ea5-79e0-4251-d0e6-bbb3e1c0b95d"
      },
      "id": "lvO4tDbp4Fj6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. canâ€™t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdoganâ€™s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summary for 2023-02-21:\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble familyâ€™s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: whoâ€™s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know wonâ€™t talk brought see. swedish search rescue team came help earthquake turkey â€” saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulationâ€”uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vuqqAxj-4GJv"
      },
      "id": "vuqqAxj-4GJv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zuEAKz-89fzV"
      },
      "id": "zuEAKz-89fzV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finding out location based on the tweet body and giving same summary to all for each day"
      ],
      "metadata": {
        "id": "Ny6Cw24sIJ9w"
      },
      "id": "Ny6Cw24sIJ9w"
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# set up Twitter API credentials\n",
        "consumer_key = \"skakU2irbEBd97RvhpBsW0sXL\"\n",
        "consumer_secret = \"Bgb8TuOlhCB0nRBpyJCMtKOLkvhuSW7D0V1EoJzf7p7McFwknf\"\n",
        "access_token = \"880363396016869376-fFRlVcD2l7sMDmpGZw3yZelKxijcwEP\"\n",
        "access_token_secret = \"l0QpT9WuWXHCV4on1C20irIzXYrtxIoHoJXHtSh7vxG4j\"\n",
        "\n",
        "# set up API authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# set up API client\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove urls\n",
        "    text = re.sub(r'\\#\\w+', '', text)  # remove hashtags\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
        "    return text\n",
        "\n",
        "# Extract relevant tweets based on keywords and dates\n",
        "start_date = datetime.now() - timedelta(days=5)\n",
        "end_date = datetime.now()\n",
        "\n",
        "# Collect tweets for each day\n",
        "for i in range((end_date - start_date).days):\n",
        "    day = start_date + timedelta(days=i)\n",
        "    date_str = day.strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    # Extract relevant tweets based on keywords\n",
        "    keywords = ['earthquake']\n",
        "    tweets = []\n",
        "    locations = []\n",
        "    for keyword in keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang='en', tweet_mode='extended', until=date_str).items(20):\n",
        "            text = tweet.full_text\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            tweets.append(preprocessed_text)\n",
        "            \n",
        "            # Extract location entities from tweet text\n",
        "            doc = nlp(preprocessed_text)\n",
        "            loc_ents = [ent for ent in doc.ents if ent.label_ == 'GPE']\n",
        "            for ent in loc_ents:\n",
        "                if ent.text not in locations:\n",
        "                    locations.append(ent.text)\n",
        "    \n",
        "    # Rank and summarize the relevant tweets using a machine learning-based model (T5)\n",
        "    input_text = ' '.join(tweets)\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=200, min_length=50, no_repeat_ngram_size=2)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    print(summary)\n",
        "\n",
        "    print(\"#############\")\n",
        "\n",
        "    # Classify summaries by location\n",
        "    location_summaries = {}\n",
        "    for location in locations:\n",
        "        if location in location_summaries:\n",
        "            location_summaries[location].append(summary)\n",
        "        else:\n",
        "            location_summaries[location] = [summary]\n",
        "    \n",
        "    # Print summaries for each location\n",
        "    for location, summaries in location_summaries.items():\n",
        "        print(f\"Summaries for {location}:\")\n",
        "        for summary in summaries:\n",
        "          print(f\"Summary for {date_str}:\")\n",
        "          print(summary)\n",
        "          print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxZBO1849f7l",
        "outputId": "78fbcb44-db52-4d2e-d1b1-c0358fa3904a"
      },
      "id": "pxZBO1849f7l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "israel bombed syria, hitting residential area damascus. 15 people killed. canâ€™t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdoganâ€™s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "#############\n",
            "Summaries for israel:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. canâ€™t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdoganâ€™s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. canâ€™t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdoganâ€™s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summaries for damascus:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. canâ€™t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdoganâ€™s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summaries for turkey:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. canâ€™t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdoganâ€™s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summaries for ðŸ‡¹:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. canâ€™t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdoganâ€™s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble familyâ€™s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "#############\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-21:\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble familyâ€™s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "\n",
            "Summaries for m6.2 - turkey:\n",
            "Summary for 2023-02-21:\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble familyâ€™s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "\n",
            "Summaries for jordan:\n",
            "Summary for 2023-02-21:\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble familyâ€™s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "\n",
            "rt @megohelie1: whoâ€™s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "#############\n",
            "Summaries for saudi arabia:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: whoâ€™s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summaries for turkey:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: whoâ€™s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: whoâ€™s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summaries for canada:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: whoâ€™s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summaries for israel:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: whoâ€™s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            ": rccg sent relief items turkey earthquake victims, know wonâ€™t talk brought see. swedish search rescue team came help earthquake turkey â€” saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulationâ€”uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "#############\n",
            "Summaries for turkey:\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know wonâ€™t talk brought see. swedish search rescue team came help earthquake turkey â€” saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulationâ€”uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know wonâ€™t talk brought see. swedish search rescue team came help earthquake turkey â€” saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulationâ€”uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            "Summaries for iran:\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know wonâ€™t talk brought see. swedish search rescue team came help earthquake turkey â€” saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulationâ€”uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            "Summaries for north east:\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know wonâ€™t talk brought see. swedish search rescue team came help earthquake turkey â€” saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulationâ€”uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "#############\n",
            "Summaries for webinar:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for indonesia:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for israel:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for damascus:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for chest korea:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for cairo:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for egypt:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for ðŸ‡¹:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for united states:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for usgs:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VqXtZ74t9oiT"
      },
      "id": "VqXtZ74t9oiT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6qB05lFIIaK"
      },
      "id": "D6qB05lFIIaK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# set up Twitter API credentials\n",
        "consumer_key = \"skakU2irbEBd97RvhpBsW0sXL\"\n",
        "consumer_secret = \"Bgb8TuOlhCB0nRBpyJCMtKOLkvhuSW7D0V1EoJzf7p7McFwknf\"\n",
        "access_token = \"880363396016869376-fFRlVcD2l7sMDmpGZw3yZelKxijcwEP\"\n",
        "access_token_secret = \"l0QpT9WuWXHCV4on1C20irIzXYrtxIoHoJXHtSh7vxG4j\"\n",
        "\n",
        "# set up API authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# set up API client\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove urls\n",
        "    text = re.sub(r'\\#\\w+\\rt', '', text)  # remove hashtags\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
        "    return text\n",
        "\n",
        "# Extract relevant tweets based on keywords and dates\n",
        "start_date = datetime.now() - timedelta(days=5)\n",
        "end_date = datetime.now()\n",
        "\n",
        "# Collect tweets for each day\n",
        "for i in range((end_date - start_date).days):\n",
        "    day = start_date + timedelta(days=i)\n",
        "    date_str = day.strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    # Extract relevant tweets based on keywords\n",
        "    keywords = ['cricket','icc']\n",
        "    tweets = []\n",
        "    primary_locations={}\n",
        "    location_summaries={}\n",
        "    for keyword in keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang='en', tweet_mode='extended', until=date_str).items(100):\n",
        "            text = tweet.full_text\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            tweets.append(preprocessed_text)\n",
        "            \n",
        "            # Extract location entities from tweet text\n",
        "            doc = nlp(preprocessed_text)\n",
        "            locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
        "            \n",
        "            if len(locations)>1:\n",
        "              locations=[locations[0]]\n",
        "\n",
        "            for location in locations:\n",
        "              if location not in primary_locations:\n",
        "                primary_locations[location]=0\n",
        "              else:\n",
        "                primary_locations[location]=primary_locations[location]+1\n",
        "\n",
        "              if location not in location_summaries:\n",
        "                location_summaries[location] = []\n",
        "              location_summaries[location].append(preprocessed_text)\n",
        "            \n",
        "    print(primary_locations)\n",
        "    max_value = max(primary_locations.values())\n",
        "    print(max_value) \n",
        "\n",
        "    threshold=math.floor(math.sqrt(max_value))\n",
        "\n",
        "    # Rank and summarize the relevant tweets using a machine learning-based model (T5)\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "    \n",
        "    #prev_sum=\"\";\n",
        "    print()\n",
        "    print()\n",
        "    print(\"#\"*20)\n",
        "    print(f\"Summary for {date_str}:\")\n",
        "    print(\"#\"*20)\n",
        "    for location, location_tweets in location_summaries.items():\n",
        "        if primary_locations[location]>=threshold:\n",
        "          input_text = ' '.join(location_tweets)\n",
        "          input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "          summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=100, min_length=10, no_repeat_ngram_size=2)\n",
        "          summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "          #if prev_sum != summary:\n",
        "          print(f\"Summaries for {location}:\")\n",
        "          # Output summary for each location\n",
        "          print(summary)\n",
        "          print()\n",
        "          #prev_sum=summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aQ6vRJ3UIIfX",
        "outputId": "04f4c9d0-0a97-41fb-cb4e-b54da2d604b9"
      },
      "id": "aQ6vRJ3UIIfX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'missouri': 0, 'pakistan': 5, 'australia': 2, 'bafta2023': 0, 'india': 10, 'pradhanmantri sangrahalaya': 3, 'rt @cfcdubois:': 7, 'ðŸ‘‘': 2, 'jodhpur center': 0, 'rt @ravishastriofc': 1, 'gujarat': 1, 'karimaakhankc': 1, 'south africa': 0, 'colonies': 0, 'israel': 0}\n",
            "10\n",
            "\n",
            "\n",
            "####################\n",
            "Summary for 2023-02-20:\n",
            "####################\n",
            "Summaries for pakistan:\n",
            "rt @gauravcsawant: 93 blast 26/11 #pulwama, terror state #pakistan bleeds india refuses punish perpetrators... women's t20 world cup digital daily: episode 16 'west indies take wire pakestan' #nzvsl.\n",
            "\n",
            "Summaries for india:\n",
            "cricket australia lot ability influence. hard right wingers understand? @ellyseperry @foxcricket nope. india pollution problem. half time aussies could see ball coming from. also @icc needs check pitch. one delhi 8 day old pitch. @sarfaraza_fan: #onthisday 2006 pakistan india u19 world cup beating india leadership champion kaptaan\n",
            "\n",
            "Summaries for pradhanmantri sangrahalaya:\n",
            "glad see indian cricket team visit pradhanmantri sangrahalaya, bipartisan initiative pm @narendramodi ji honour indiaâ€™s prime ministers. @_nmml continue work improve user experience pm sangranathalaya.\n",
            "\n",
            "Summaries for rt @cfcdubois::\n",
            "rt @cfcdubois: potter miraculously gets sacked replace poch genuinely might rebrand cricket account..\n",
            "\n",
            "{'australia': 9, 'united kingdom': 1, 'india': 16, 'khan': 0, '@ravishastriofc': 0, 'rt @cfcdubois:': 0, 'rt @ravishastriofc': 0, 'mumbai': 1, 'united states': 0, 'rome': 0, 'philippines': 0, 'icc united states': 0, 'america': 0, 'venezuela': 1, 'estrada': 0, 'karimaakhankc': 0, '@icc_buikem @adeyimika0': 0}\n",
            "16\n",
            "\n",
            "\n",
            "####################\n",
            "Summary for 2023-02-21:\n",
            "####################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries for australia:\n",
            "@cricketcomau cricket australia highlighting shocking umpiring decision  australia consider sending players home india look downsize squad ahead final two tests. #7news rt @lenoretaylor: go wrong australiaâ€™s test cricket team tour india? - hope fine similar kind treatment indian gets australia. no? please look take necessary actions. hooliganism, nothing nationalism.\n",
            "\n",
            "Summaries for india:\n",
            "rt @phadke_shilpa: hey @bcci @jayshah - past 14 months, cricket spectator 2 india's biggest cities, along m... @icc except davey, none injured onfield bgt matches.. with wtc final dreams line, eyes india right ship, game time! #bordergavaskartrophy2023 #indvaus #cricke\n",
            "\n",
            "{'india': 5, 'washington': 0, 'sri': 4, 'spain': 0, 'south africa': 7, 'pakistan': 5, 'namibia': 0, 'ðŸ‡¿': 0, 'philippines': 0, 'england': 0, 'nigeria': 0, 'rome': 0, 'myeloid': 1}\n",
            "7\n",
            "\n",
            "\n",
            "####################\n",
            "Summary for 2023-02-22:\n",
            "####################\n",
            "Summaries for india:\n",
            "@icc: india's richa ghosh enters top-20 #t20womensworldcup batters read: #richaghosh. kl rahul bcci bringing minor point. selection team based performance...nothing else. @harbhajan_singh @bhogleharsha many disagree harsha. india plays cricket round year. saying end test match 48hours\n",
            "\n",
            "Summaries for sri:\n",
            "rex clementine usually, australians town, look forward cricket play hard hard tough bunch well-equipped come top demanding conditions. sri lanka cricket board no... rt @areeb_7official: big breaking news. big blow quetta gladiators wanindu hasaranga psl 2023.\n",
            "\n",
            "Summaries for south africa:\n",
            "rt @icc: south africa reach semi-finals : #savban | #t20worldcup | tazmin brits chased by laura wolvaardt. nathaniel clyne: england bowled out for 111 in a row, with sam burgess taking the wickets of chris rob\n",
            "\n",
            "Summaries for pakistan:\n",
            "rt @mughalbha: 1 - cricket pakistan: nation, identity, politics mr @nadeemfparacha. ahmer nqvi @karachikhatmal &amp; others #kl... tt: sarfasays_: agenda driven mafia campaigned replace babar...\n",
            "\n",
            "{'rt @bai_mina': 0, 'ðŸ‘‘': 0, 'gujarati': 1, 'india': 8, 'england': 3, 'australia': 5, 'saudi arabia': 0, 'new zealand': 0, 'pakistan': 0, 'uk': 0, 'no.7': 1, 'florida': 0, 'nigeria': 0, 'rt @peterobi:': 37, 'norway': 0, 'h.e.': 0, 'ejercito': 0}\n",
            "37\n",
            "\n",
            "\n",
            "####################\n",
            "Summary for 2023-02-23:\n",
            "####################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1255 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries for india:\n",
            "rt @cricketracker: sachin tendulkar virat kohli india batters score 25,000 runs international cricket.  #icctestrankings | #allrounders india - a great place to be if you want to know more about cricket in india, listen regular cricket etc podcast @plalor &amp; gideon haigh... india's dominance world cricket! #cric\n",
            "\n",
            "Summaries for rt @peterobi::\n",
            "lp national chairman barr julius abure. candidates signed... rt @peterobi: signed 2nd peace accord icc along spnc. css: msc:mb:psh:gp:sk:oba:tbb; bsg:br:ksd:nsw:dsl:lsm:abure\n",
            "\n",
            "{'@yeahitsjin': 0, 'healy': 2, 'new zealand - england': 1, 'australia': 4, 'india': 12, 'paris': 0, 'new zealand': 1, 'n zeland': 0, 'pakistan': 0, 'rt @peterobi:': 6, 'ukraine': 0, 'mumbai': 0, 'israel': 0, 'karimaakhankc': 0, 'rome': 0}\n",
            "12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-d875ca07641b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Rank and summarize the relevant tweets using a machine learning-based model (T5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m#prev_sum=\"\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0mencoder_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0mencoder_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mdecoder_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         self.block = nn.ModuleList(\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mT5Block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5LayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         self.block = nn.ModuleList(\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mT5Block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5LayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT5LayerSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT5LayerCrossAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSelfAttention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5LayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mt5xYCdWIldD"
      },
      "id": "mt5xYCdWIldD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}