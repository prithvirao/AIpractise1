{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prithvirao/AIpractise1/blob/master/NLP_project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019bd668",
      "metadata": {
        "id": "019bd668"
      },
      "outputs": [],
      "source": [
        " import tweepy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tweepy==3.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "7DOllOQxzonX",
        "outputId": "c839a4a4-f673-4a89-8f91-610833ae9b74"
      },
      "id": "7DOllOQxzonX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweepy==3.7.0\n",
            "  Downloading tweepy-3.7.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.8/dist-packages (from tweepy==3.7.0) (1.7.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tweepy==3.7.0) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.8/dist-packages (from tweepy==3.7.0) (2.25.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tweepy==3.7.0) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.11.1->tweepy==3.7.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.11.1->tweepy==3.7.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.11.1->tweepy==3.7.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.11.1->tweepy==3.7.0) (4.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->tweepy==3.7.0) (3.2.2)\n",
            "Installing collected packages: tweepy\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "Successfully installed tweepy-3.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tweepy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kyhp-Z6lznCC"
      },
      "id": "Kyhp-Z6lznCC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8be5918",
      "metadata": {
        "id": "b8be5918"
      },
      "outputs": [],
      "source": [
        "# set up Twitter API credentials\n",
        "consumer_key = \"skakU2irbEBd97RvhpBsW0sXL\"\n",
        "consumer_secret = \"Bgb8TuOlhCB0nRBpyJCMtKOLkvhuSW7D0V1EoJzf7p7McFwknf\"\n",
        "access_token = \"880363396016869376-fFRlVcD2l7sMDmpGZw3yZelKxijcwEP\"\n",
        "access_token_secret = \"l0QpT9WuWXHCV4on1C20irIzXYrtxIoHoJXHtSh7vxG4j\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3686fa",
      "metadata": {
        "id": "dc3686fa"
      },
      "outputs": [],
      "source": [
        "# set up API authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35ebc17",
      "metadata": {
        "id": "f35ebc17"
      },
      "outputs": [],
      "source": [
        "# set up API client\n",
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707ccfa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "707ccfa4",
        "outputId": "e8094b19-08b5-4f86-c168-8d94d1202b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a0530d4",
      "metadata": {
        "id": "9a0530d4"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove urls\n",
        "    text = re.sub(r'\\#\\w+', '', text)  # remove hashtags\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "TjoqRaWo1Y7X"
      },
      "id": "TjoqRaWo1Y7X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737b2980",
      "metadata": {
        "id": "737b2980"
      },
      "outputs": [],
      "source": [
        "# Extract relevant tweets based on keywords and retweet count\n",
        "keywords = ['earthquake']\n",
        "retweet_threshold = 5\n",
        "# Define the date range\n",
        "#start_date = datetime.utcnow() - timedelta(days=5)\n",
        "#end_date = datetime.utcnow()\n",
        "today = datetime.datetime.now().date()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-U6k5Nb0DOM",
        "outputId": "ac87693f-64c5-4ffe-e207-a6880adfe207"
      },
      "id": "0-U6k5Nb0DOM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "GDf5KA7i0eq2",
        "outputId": "47e723a6-12de-44bc-eb59-10a49989bba7"
      },
      "id": "GDf5KA7i0eq2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.26.1\n",
            "Uninstalling transformers-4.26.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.8/dist-packages/transformers-4.26.1.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/transformers/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.26.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb27ec66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "cb27ec66",
        "outputId": "a657ff22-b8b7-46e9-a318-2c7de85db55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of earthquake related tweets for 2023-02-23:\n",
            "; a d o é, e. : - s à  )'en?!... & » / _ ---   de l y ant  au eau  tub tub ul  in t  do re n f er  and oxioxi aliment  for ß procédé essai  to épreuve he actu m c eur  ( );\n",
            "\n",
            "\n",
            "Summary of earthquake related tweets for 2023-02-22:\n",
            "; a d o é, e. : - s à  )'en?!... & » / _ ---   de l y ant  au eau  tub tub ul  in t  do re n f er  and oxioxi aliment  for ß procédé essai  to épreuve he actu m c eur  ( );\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c9cae42e21c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msummary_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m             )\n\u001b[1;32m   1473\u001b[0m             \u001b[0;31m# 13. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1475\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2720\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2723\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1689\u001b[0m             \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    date = today - datetime.timedelta(days=i+1)\n",
        "    since_date = str(date) + ' 00:00:00'\n",
        "    until_date = str(date) + ' 23:59:59'\n",
        "    tweets = []\n",
        "    for keyword in keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang='en', tweet_mode='extended', since=since_date, until=until_date).items(100):\n",
        "            text = tweet.full_text\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            if tweet.retweet_count >= retweet_threshold:\n",
        "                tweets.append((preprocessed_text, tweet.retweet_count))\n",
        "            else:\n",
        "                tweets.append((preprocessed_text, 0))\n",
        "\n",
        "    # Rank and summarize the relevant tweets using a machine learning-based model (T5)\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "    sorted_tweets = sorted(tweets, key=lambda x: x[1], reverse=True)  # sort by retweet count\n",
        "\n",
        "    input_text = ' '.join([tweet[0] for tweet in sorted_tweets])\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=200, min_length=50, no_repeat_ngram_size=2)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f'Summary of earthquake related tweets for {since_date.split()[0]}:')\n",
        "    print(summary)\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097d6a6d",
      "metadata": {
        "id": "097d6a6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "giving summary for each day"
      ],
      "metadata": {
        "id": "IiydfQmPIRVN"
      },
      "id": "IiydfQmPIRVN"
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# set up Twitter API credentials\n",
        "consumer_key = \"skakU2irbEBd97RvhpBsW0sXL\"\n",
        "consumer_secret = \"Bgb8TuOlhCB0nRBpyJCMtKOLkvhuSW7D0V1EoJzf7p7McFwknf\"\n",
        "access_token = \"880363396016869376-fFRlVcD2l7sMDmpGZw3yZelKxijcwEP\"\n",
        "access_token_secret = \"l0QpT9WuWXHCV4on1C20irIzXYrtxIoHoJXHtSh7vxG4j\"\n",
        "\n",
        "# set up API authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# set up API client\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove urls\n",
        "    text = re.sub(r'\\#\\w+', '', text)  # remove hashtags\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
        "    return text\n",
        "\n",
        "# Extract relevant tweets based on keywords and dates\n",
        "start_date = datetime.now() - timedelta(days=5)\n",
        "end_date = datetime.now()\n",
        "\n",
        "# Collect tweets for each day\n",
        "for i in range((end_date - start_date).days):\n",
        "    day = start_date + timedelta(days=i)\n",
        "    date_str = day.strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    # Extract relevant tweets based on keywords\n",
        "    keywords = ['earthquake']\n",
        "    tweets = []\n",
        "    for keyword in keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang='en', tweet_mode='extended', until=date_str).items(20):\n",
        "            text = tweet.full_text\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            tweets.append(preprocessed_text)\n",
        "    \n",
        "    # Rank and summarize the relevant tweets using a machine learning-based model (T5)\n",
        "    input_text = ' '.join(tweets)\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=200, min_length=50, no_repeat_ngram_size=2)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"Summary for {date_str}:\")\n",
        "    print(summary)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvO4tDbp4Fj6",
        "outputId": "c6804ea5-79e0-4251-d0e6-bbb3e1c0b95d"
      },
      "id": "lvO4tDbp4Fj6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. can’t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdogan’s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summary for 2023-02-21:\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble family’s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: who’s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know won’t talk brought see. swedish search rescue team came help earthquake turkey — saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulation—uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vuqqAxj-4GJv"
      },
      "id": "vuqqAxj-4GJv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zuEAKz-89fzV"
      },
      "id": "zuEAKz-89fzV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finding out location based on the tweet body and giving same summary to all for each day"
      ],
      "metadata": {
        "id": "Ny6Cw24sIJ9w"
      },
      "id": "Ny6Cw24sIJ9w"
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# set up Twitter API credentials\n",
        "consumer_key = \"skakU2irbEBd97RvhpBsW0sXL\"\n",
        "consumer_secret = \"Bgb8TuOlhCB0nRBpyJCMtKOLkvhuSW7D0V1EoJzf7p7McFwknf\"\n",
        "access_token = \"880363396016869376-fFRlVcD2l7sMDmpGZw3yZelKxijcwEP\"\n",
        "access_token_secret = \"l0QpT9WuWXHCV4on1C20irIzXYrtxIoHoJXHtSh7vxG4j\"\n",
        "\n",
        "# set up API authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# set up API client\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove urls\n",
        "    text = re.sub(r'\\#\\w+', '', text)  # remove hashtags\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
        "    return text\n",
        "\n",
        "# Extract relevant tweets based on keywords and dates\n",
        "start_date = datetime.now() - timedelta(days=5)\n",
        "end_date = datetime.now()\n",
        "\n",
        "# Collect tweets for each day\n",
        "for i in range((end_date - start_date).days):\n",
        "    day = start_date + timedelta(days=i)\n",
        "    date_str = day.strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    # Extract relevant tweets based on keywords\n",
        "    keywords = ['earthquake']\n",
        "    tweets = []\n",
        "    locations = []\n",
        "    for keyword in keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang='en', tweet_mode='extended', until=date_str).items(20):\n",
        "            text = tweet.full_text\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            tweets.append(preprocessed_text)\n",
        "            \n",
        "            # Extract location entities from tweet text\n",
        "            doc = nlp(preprocessed_text)\n",
        "            loc_ents = [ent for ent in doc.ents if ent.label_ == 'GPE']\n",
        "            for ent in loc_ents:\n",
        "                if ent.text not in locations:\n",
        "                    locations.append(ent.text)\n",
        "    \n",
        "    # Rank and summarize the relevant tweets using a machine learning-based model (T5)\n",
        "    input_text = ' '.join(tweets)\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=200, min_length=50, no_repeat_ngram_size=2)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    print(summary)\n",
        "\n",
        "    print(\"#############\")\n",
        "\n",
        "    # Classify summaries by location\n",
        "    location_summaries = {}\n",
        "    for location in locations:\n",
        "        if location in location_summaries:\n",
        "            location_summaries[location].append(summary)\n",
        "        else:\n",
        "            location_summaries[location] = [summary]\n",
        "    \n",
        "    # Print summaries for each location\n",
        "    for location, summaries in location_summaries.items():\n",
        "        print(f\"Summaries for {location}:\")\n",
        "        for summary in summaries:\n",
        "          print(f\"Summary for {date_str}:\")\n",
        "          print(summary)\n",
        "          print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxZBO1849f7l",
        "outputId": "78fbcb44-db52-4d2e-d1b1-c0358fa3904a"
      },
      "id": "pxZBO1849f7l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "israel bombed syria, hitting residential area damascus. 15 people killed. can’t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdogan’s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "#############\n",
            "Summaries for israel:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. can’t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdogan’s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. can’t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdogan’s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summaries for damascus:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. can’t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdogan’s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summaries for turkey:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. can’t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdogan’s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "Summaries for 🇹:\n",
            "Summary for 2023-02-20:\n",
            "israel bombed syria, hitting residential area damascus. 15 people killed. can’t leave people peace even i... might need pass new legislation waste time. rt @dailyloud: \"israel enjoying free jolly palestinian people's misery\" erdogan’s government allowed shoddy construction bcuz profit important lives. turkey seriously needs wake up!\n",
            "\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble family’s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "#############\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-21:\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble family’s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "\n",
            "Summaries for m6.2 - turkey:\n",
            "Summary for 2023-02-21:\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble family’s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "\n",
            "Summaries for jordan:\n",
            "Summary for 2023-02-21:\n",
            "new earthquake measuring 6.4 occurred followed aftershock. heart goes ou... rt @alhamdhulillaah: baby girl whose story widely shared news reports born rubble family’s earthquake-shatt. subhanallah, destruction death. heard another earthquake hit hatay ( turkiye ) hours ago. really wish could them. please say prayer come across this. sighh.\n",
            "\n",
            "rt @megohelie1: who’s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "#############\n",
            "Summaries for saudi arabia:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: who’s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summaries for turkey:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: who’s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: who’s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summaries for canada:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: who’s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            "Summaries for israel:\n",
            "Summary for 2023-02-22:\n",
            "rt @megohelie1: who’s funding requirement us$ 84.57 million address evolving health needs devasta. authorities say 110,000 buildings or destroyed severely damaged need torn down, estimated 45k people died? lokanta mediterranean grill &amp; bar donate sales made wednesday help victims earthquake turkey syria?\n",
            "\n",
            ": rccg sent relief items turkey earthquake victims, know won’t talk brought see. swedish search rescue team came help earthquake turkey — saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulation—uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "#############\n",
            "Summaries for turkey:\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know won’t talk brought see. swedish search rescue team came help earthquake turkey — saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulation—uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know won’t talk brought see. swedish search rescue team came help earthquake turkey — saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulation—uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            "Summaries for iran:\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know won’t talk brought see. swedish search rescue team came help earthquake turkey — saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulation—uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            "Summaries for north east:\n",
            "Summary for 2023-02-23:\n",
            ": rccg sent relief items turkey earthquake victims, know won’t talk brought see. swedish search rescue team came help earthquake turkey — saved 10 lives turkey. smash the... buried secrets behind worst earthquake decade via @yahoo nn3: haarp weather manipulation—uss m2.7 occurred 13 km se () 22 min ago (local time 02:24:28). despite disaster earthquake, dictator plans launch invasion operation (north east) - u think early go get\n",
            "\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "#############\n",
            "Summaries for webinar:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for indonesia:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for syria:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for israel:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for damascus:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for chest korea:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for cairo:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for egypt:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for 🇹:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for united states:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n",
            "Summaries for usgs:\n",
            "Summary for 2023-02-24:\n",
            ": 6.3 earthquake hit 177 km n tobelo, indonesia - usgs rt @abc3340: death toll massive earthquake occurred 560km fiji 2018. continues deform earth years after, 1000s miles... click support urgent! turkey syrian earthquake relief organized united crisis relief : animal rescue team turkey now, providing direct care injured animals &amp; supporting loca...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VqXtZ74t9oiT"
      },
      "id": "VqXtZ74t9oiT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6qB05lFIIaK"
      },
      "id": "D6qB05lFIIaK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# set up Twitter API credentials\n",
        "consumer_key = \"skakU2irbEBd97RvhpBsW0sXL\"\n",
        "consumer_secret = \"Bgb8TuOlhCB0nRBpyJCMtKOLkvhuSW7D0V1EoJzf7p7McFwknf\"\n",
        "access_token = \"880363396016869376-fFRlVcD2l7sMDmpGZw3yZelKxijcwEP\"\n",
        "access_token_secret = \"l0QpT9WuWXHCV4on1C20irIzXYrtxIoHoJXHtSh7vxG4j\"\n",
        "\n",
        "# set up API authentication\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# set up API client\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove urls\n",
        "    text = re.sub(r'\\#\\w+\\rt', '', text)  # remove hashtags\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
        "    return text\n",
        "\n",
        "# Extract relevant tweets based on keywords and dates\n",
        "start_date = datetime.now() - timedelta(days=5)\n",
        "end_date = datetime.now()\n",
        "\n",
        "# Collect tweets for each day\n",
        "for i in range((end_date - start_date).days):\n",
        "    day = start_date + timedelta(days=i)\n",
        "    date_str = day.strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    # Extract relevant tweets based on keywords\n",
        "    keywords = ['cricket','icc']\n",
        "    tweets = []\n",
        "    primary_locations={}\n",
        "    location_summaries={}\n",
        "    for keyword in keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang='en', tweet_mode='extended', until=date_str).items(100):\n",
        "            text = tweet.full_text\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            tweets.append(preprocessed_text)\n",
        "            \n",
        "            # Extract location entities from tweet text\n",
        "            doc = nlp(preprocessed_text)\n",
        "            locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
        "            \n",
        "            if len(locations)>1:\n",
        "              locations=[locations[0]]\n",
        "\n",
        "            for location in locations:\n",
        "              if location not in primary_locations:\n",
        "                primary_locations[location]=0\n",
        "              else:\n",
        "                primary_locations[location]=primary_locations[location]+1\n",
        "\n",
        "              if location not in location_summaries:\n",
        "                location_summaries[location] = []\n",
        "              location_summaries[location].append(preprocessed_text)\n",
        "            \n",
        "    print(primary_locations)\n",
        "    max_value = max(primary_locations.values())\n",
        "    print(max_value) \n",
        "\n",
        "    threshold=math.floor(math.sqrt(max_value))\n",
        "\n",
        "    # Rank and summarize the relevant tweets using a machine learning-based model (T5)\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "    \n",
        "    #prev_sum=\"\";\n",
        "    print()\n",
        "    print()\n",
        "    print(\"#\"*20)\n",
        "    print(f\"Summary for {date_str}:\")\n",
        "    print(\"#\"*20)\n",
        "    for location, location_tweets in location_summaries.items():\n",
        "        if primary_locations[location]>=threshold:\n",
        "          input_text = ' '.join(location_tweets)\n",
        "          input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "          summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=100, min_length=10, no_repeat_ngram_size=2)\n",
        "          summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "          #if prev_sum != summary:\n",
        "          print(f\"Summaries for {location}:\")\n",
        "          # Output summary for each location\n",
        "          print(summary)\n",
        "          print()\n",
        "          #prev_sum=summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aQ6vRJ3UIIfX",
        "outputId": "04f4c9d0-0a97-41fb-cb4e-b54da2d604b9"
      },
      "id": "aQ6vRJ3UIIfX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'missouri': 0, 'pakistan': 5, 'australia': 2, 'bafta2023': 0, 'india': 10, 'pradhanmantri sangrahalaya': 3, 'rt @cfcdubois:': 7, '👑': 2, 'jodhpur center': 0, 'rt @ravishastriofc': 1, 'gujarat': 1, 'karimaakhankc': 1, 'south africa': 0, 'colonies': 0, 'israel': 0}\n",
            "10\n",
            "\n",
            "\n",
            "####################\n",
            "Summary for 2023-02-20:\n",
            "####################\n",
            "Summaries for pakistan:\n",
            "rt @gauravcsawant: 93 blast 26/11 #pulwama, terror state #pakistan bleeds india refuses punish perpetrators... women's t20 world cup digital daily: episode 16 'west indies take wire pakestan' #nzvsl.\n",
            "\n",
            "Summaries for india:\n",
            "cricket australia lot ability influence. hard right wingers understand? @ellyseperry @foxcricket nope. india pollution problem. half time aussies could see ball coming from. also @icc needs check pitch. one delhi 8 day old pitch. @sarfaraza_fan: #onthisday 2006 pakistan india u19 world cup beating india leadership champion kaptaan\n",
            "\n",
            "Summaries for pradhanmantri sangrahalaya:\n",
            "glad see indian cricket team visit pradhanmantri sangrahalaya, bipartisan initiative pm @narendramodi ji honour india’s prime ministers. @_nmml continue work improve user experience pm sangranathalaya.\n",
            "\n",
            "Summaries for rt @cfcdubois::\n",
            "rt @cfcdubois: potter miraculously gets sacked replace poch genuinely might rebrand cricket account..\n",
            "\n",
            "{'australia': 9, 'united kingdom': 1, 'india': 16, 'khan': 0, '@ravishastriofc': 0, 'rt @cfcdubois:': 0, 'rt @ravishastriofc': 0, 'mumbai': 1, 'united states': 0, 'rome': 0, 'philippines': 0, 'icc united states': 0, 'america': 0, 'venezuela': 1, 'estrada': 0, 'karimaakhankc': 0, '@icc_buikem @adeyimika0': 0}\n",
            "16\n",
            "\n",
            "\n",
            "####################\n",
            "Summary for 2023-02-21:\n",
            "####################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries for australia:\n",
            "@cricketcomau cricket australia highlighting shocking umpiring decision  australia consider sending players home india look downsize squad ahead final two tests. #7news rt @lenoretaylor: go wrong australia’s test cricket team tour india? - hope fine similar kind treatment indian gets australia. no? please look take necessary actions. hooliganism, nothing nationalism.\n",
            "\n",
            "Summaries for india:\n",
            "rt @phadke_shilpa: hey @bcci @jayshah - past 14 months, cricket spectator 2 india's biggest cities, along m... @icc except davey, none injured onfield bgt matches.. with wtc final dreams line, eyes india right ship, game time! #bordergavaskartrophy2023 #indvaus #cricke\n",
            "\n",
            "{'india': 5, 'washington': 0, 'sri': 4, 'spain': 0, 'south africa': 7, 'pakistan': 5, 'namibia': 0, '🇿': 0, 'philippines': 0, 'england': 0, 'nigeria': 0, 'rome': 0, 'myeloid': 1}\n",
            "7\n",
            "\n",
            "\n",
            "####################\n",
            "Summary for 2023-02-22:\n",
            "####################\n",
            "Summaries for india:\n",
            "@icc: india's richa ghosh enters top-20 #t20womensworldcup batters read: #richaghosh. kl rahul bcci bringing minor point. selection team based performance...nothing else. @harbhajan_singh @bhogleharsha many disagree harsha. india plays cricket round year. saying end test match 48hours\n",
            "\n",
            "Summaries for sri:\n",
            "rex clementine usually, australians town, look forward cricket play hard hard tough bunch well-equipped come top demanding conditions. sri lanka cricket board no... rt @areeb_7official: big breaking news. big blow quetta gladiators wanindu hasaranga psl 2023.\n",
            "\n",
            "Summaries for south africa:\n",
            "rt @icc: south africa reach semi-finals : #savban | #t20worldcup | tazmin brits chased by laura wolvaardt. nathaniel clyne: england bowled out for 111 in a row, with sam burgess taking the wickets of chris rob\n",
            "\n",
            "Summaries for pakistan:\n",
            "rt @mughalbha: 1 - cricket pakistan: nation, identity, politics mr @nadeemfparacha. ahmer nqvi @karachikhatmal &amp; others #kl... tt: sarfasays_: agenda driven mafia campaigned replace babar...\n",
            "\n",
            "{'rt @bai_mina': 0, '👑': 0, 'gujarati': 1, 'india': 8, 'england': 3, 'australia': 5, 'saudi arabia': 0, 'new zealand': 0, 'pakistan': 0, 'uk': 0, 'no.7': 1, 'florida': 0, 'nigeria': 0, 'rt @peterobi:': 37, 'norway': 0, 'h.e.': 0, 'ejercito': 0}\n",
            "37\n",
            "\n",
            "\n",
            "####################\n",
            "Summary for 2023-02-23:\n",
            "####################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1255 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries for india:\n",
            "rt @cricketracker: sachin tendulkar virat kohli india batters score 25,000 runs international cricket.  #icctestrankings | #allrounders india - a great place to be if you want to know more about cricket in india, listen regular cricket etc podcast @plalor &amp; gideon haigh... india's dominance world cricket! #cric\n",
            "\n",
            "Summaries for rt @peterobi::\n",
            "lp national chairman barr julius abure. candidates signed... rt @peterobi: signed 2nd peace accord icc along spnc. css: msc:mb:psh:gp:sk:oba:tbb; bsg:br:ksd:nsw:dsl:lsm:abure\n",
            "\n",
            "{'@yeahitsjin': 0, 'healy': 2, 'new zealand - england': 1, 'australia': 4, 'india': 12, 'paris': 0, 'new zealand': 1, 'n zeland': 0, 'pakistan': 0, 'rt @peterobi:': 6, 'ukraine': 0, 'mumbai': 0, 'israel': 0, 'karimaakhankc': 0, 'rome': 0}\n",
            "12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-d875ca07641b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Rank and summarize the relevant tweets using a machine learning-based model (T5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m#prev_sum=\"\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0mencoder_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0mencoder_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mdecoder_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         self.block = nn.ModuleList(\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mT5Block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5LayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         self.block = nn.ModuleList(\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mT5Block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5LayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT5LayerSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT5LayerCrossAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSelfAttention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5LayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mt5xYCdWIldD"
      },
      "id": "mt5xYCdWIldD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}